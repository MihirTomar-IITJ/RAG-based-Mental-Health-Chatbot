{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "67fab0e1",
            "metadata": {},
            "source": [
                "# RAG-based Mental Health Chatbot\n",
                "\n",
                "This notebook implements a RAG (Retrieval-Augmented Generation) chatbot for mental health support using Google Gemini and LangChain."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f65e7947",
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -q -U langchain langchain-google-genai langchain-community pypdf faiss-cpu google-generativeai langchain-huggingface sentence-transformers chainlit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c95aba2d",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "from langchain_community.vectorstores import FAISS\n",
                "from langchain.chains import RetrievalQA\n",
                "from langchain.prompts import PromptTemplate\n",
                "\n",
                "# Set API Keys\n",
                "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCPToG3uYMof94dduZ2W0nfjQSXPz6Gyz4\"\n",
                "# os.environ[\"HF_TOKEN\"] = \"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\" # Not strictly needed for local embeddings"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "54c14524",
            "metadata": {},
            "source": [
                "## 1. Load Data\n",
                "Loading PDF documents from the `content/data` directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2539da63",
            "metadata": {},
            "outputs": [],
            "source": [
                "pdf_directory = 'content/data'\n",
                "documents = []\n",
                "\n",
                "if os.path.exists(pdf_directory):\n",
                "    for file in os.listdir(pdf_directory):\n",
                "        if file.endswith('.pdf'):\n",
                "            file_path = os.path.join(pdf_directory, file)\n",
                "            print(f\"Loading {file_path}...\")\n",
                "            loader = PyPDFLoader(file_path)\n",
                "            documents.extend(loader.load())\n",
                "    print(f\"Total documents loaded: {len(documents)}\")\n",
                "else:\n",
                "    print(f\"Directory {pdf_directory} not found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "04895435",
            "metadata": {},
            "source": [
                "## 2. Process Text\n",
                "Split the loaded documents into smaller chunks for embedding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dc31ee63",
            "metadata": {},
            "outputs": [],
            "source": [
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "texts = text_splitter.split_documents(documents)\n",
                "print(f\"Total text chunks: {len(texts)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6d51526b",
            "metadata": {},
            "source": [
                "## 3. Create Vector Store\n",
                "Generate embeddings using HuggingFace's `all-MiniLM-L6-v2` model and store them in a FAISS index."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1a334fbc",
            "metadata": {},
            "outputs": [],
            "source": [
                "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "\n",
                "# Create FAISS vector store\n",
                "vector_store = FAISS.from_documents(texts, embeddings)\n",
                "\n",
                "# Save locally\n",
                "vector_store.save_local(\"faiss_index_local\")\n",
                "print(\"Vector store created and saved.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "92309da0",
            "metadata": {},
            "source": [
                "## 4. Setup Retrieval Chain\n",
                "Configure the RAG chain using the Gemini Pro model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d8840be",
            "metadata": {},
            "outputs": [],
            "source": [
                "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
                "\n",
                "prompt_template = \"\"\"You are an empathetic and professional mental health chatbot assistant.\n",
                "Use the following pieces of context to answer the user's question. \n",
                "If the answer is not in the context, generally answer from your knowledge but mention that this specific information wasn't in the provided documents.\n",
                "Always maintain a supportive tone.\n",
                "\n",
                "Context: {context}\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\"\n",
                "\n",
                "PROMPT = PromptTemplate(\n",
                "    template=prompt_template, \n",
                "    input_variables=[\"context\", \"question\"]\n",
                ")\n",
                "\n",
                "qa_chain = RetrievalQA.from_chain_type(\n",
                "    llm=llm,\n",
                "    chain_type=\"stuff\",\n",
                "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 5}),\n",
                "    chain_type_kwargs={\"prompt\": PROMPT}\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "30646b64",
            "metadata": {},
            "source": [
                "## 5. Test the Chatbot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e61976b",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"What are some common signs of anxiety?\"\n",
                "result = qa_chain.invoke(query)\n",
                "print(\"Q:\", query)\n",
                "print(\"A:\", result['result'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"How can I help a friend who is depressed?\"\n",
                "result = qa_chain.invoke(query)\n",
                "print(\"Q:\", query)\n",
                "print(\"A:\", result['result'])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eaa45c4d",
            "metadata": {},
            "source": [
                "## 6. Create Chainlit App Interface\n",
                "Run the following cell to create the `app.py` file. You can then run it from the terminal using:\n",
                "`chainlit run app.py -w`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3890679b",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile app.py\n",
                "import os\n",
                "import chainlit as cl\n",
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "from langchain_community.vectorstores import FAISS\n",
                "from langchain.prompts import PromptTemplate\n",
                "from langchain.chains import RetrievalQA\n",
                "\n",
                "# Set API Keys\n",
                "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCPToG3uYMof94dduZ2W0nfjQSXPz6Gyz4\"\n",
                "\n",
                "# Load Embeddings\n",
                "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "\n",
                "# Load Vector Store\n",
                "# Make sure 'faiss_index_local' was generated by the notebook cells above\n",
                "vector_store = FAISS.load_local(\"faiss_index_local\", embeddings, allow_dangerous_deserialization=True)\n",
                "\n",
                "# Setup LLM\n",
                "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
                "\n",
                "prompt_template = \"\"\"You are an empathetic and professional mental health chatbot assistant.\n",
                "Use the following pieces of context to answer the user's question. \n",
                "If the answer is not in the context, generally answer from your knowledge but mention that this specific information wasn't in the provided documents.\n",
                "Always maintain a supportive tone.\n",
                "\n",
                "Context: {context}\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\"\n",
                "\n",
                "PROMPT = PromptTemplate(\n",
                "    template=prompt_template, \n",
                "    input_variables=[\"context\", \"question\"]\n",
                ")\n",
                "\n",
                "@cl.on_chat_start\n",
                "async def start():\n",
                "    chain = RetrievalQA.from_chain_type(\n",
                "        llm=llm,\n",
                "        chain_type=\"stuff\",\n",
                "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 5}),\n",
                "        chain_type_kwargs={\"prompt\": PROMPT}\n",
                "    )\n",
                "    cl.user_session.set(\"chain\", chain)\n",
                "    await cl.Message(content=\"Hello! I'm here to support you. How are you feeling today?\").send()\n",
                "\n",
                "@cl.on_message\n",
                "async def main(message: cl.Message):\n",
                "    chain = cl.user_session.get(\"chain\")\n",
                "    # Async callback handler for streaming if supported, though RetrievalQA is synchronous by default unless using .acall or similar with async components\n",
                "    res = await chain.ainvoke(message.content)\n",
                "    answer = res[\"result\"]\n",
                "    \n",
                "    await cl.Message(content=answer).send()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
