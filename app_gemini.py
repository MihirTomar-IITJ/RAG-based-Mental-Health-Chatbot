from langchain.chains import LLMChain
import chainlit as cl
import os
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.memory import ConversationBufferWindowMemory
from langchain.callbacks import StdOutCallbackHandler

# Set API keys
os.environ["GOOGLE_API_KEY"] = "AIzaSyCPToG3uYMof94dduZ2W0nfjQSXPz6Gyz4"

# Path to the vector store generated by Chatbot.ipynb
VECTOR_STORE_PATH = "./faiss_index_gemini"

@cl.cache
def instantiate():
    # Use Google Gemini Pro
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.3)
    
    # Use Google Embeddings
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

    # Load vector store
    # Note: Requires running Chatbot.ipynb first to generate this index
    if os.path.exists(VECTOR_STORE_PATH):
        index = FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True)
    else:
        print(f"Warning: Vector store not found at {VECTOR_STORE_PATH}. Please run Chatbot.ipynb first.")
        index = None
        
    return llm, index

llm, vector_store = instantiate()

# Set up conversation chain
@cl.on_chat_start
def main():
    cl.user_session.set(
        "message_history",
        [{"role": "system", "content": "You are an empathetic mental health counselor focused on providing support and guidance."}],
    )
    
    prompt_template = """Answer the question as a supportive and empathetic mental health counselor. Always maintain professional boundaries and encourage seeking professional help when appropriate.
    Context: {context}

    {chat_history}
    Human: {question}
    Assistant:"""

    # Create prompt from prompt template 
    prompt = PromptTemplate(
        input_variables=["context", "question", "chat_history"],
        template=prompt_template,
    )

    memory = ConversationBufferWindowMemory(memory_key="chat_history", input_key="question", k=5)
    
    llm_chain = LLMChain(
        llm=llm,
        prompt=prompt,
        verbose=True,
        memory=memory
    )

    cl.user_session.set("llm_chain", llm_chain)
    
    if vector_store:
        retriever = vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={'k': 5}
        )
        cl.user_session.set("retriever", retriever)
    else:
        cl.user_session.set("retriever", None)


@cl.on_message
async def main(message: cl.Message):
    llm_chain = cl.user_session.get("llm_chain")
    retriever = cl.user_session.get("retriever")
    
    # Get the user's message
    user_input = message.content
    
    docs = []
    if retriever:
        docs = retriever.get_relevant_documents(user_input)
    
    msg = cl.Message(content="")
    await msg.send()
    
    # Generate response using LLMChain
    response = await llm_chain.arun(question=user_input, context=docs, callbacks=[cl.LangchainCallbackHandler(stream_final_answer=True)])
    
    msg.content = response
    await msg.update()
