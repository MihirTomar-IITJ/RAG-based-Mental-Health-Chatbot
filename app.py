import os
import chainlit as cl
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_classic.prompts import PromptTemplate
from langchain_classic.chains import RetrievalQA

# Set API Keys
os.environ["GOOGLE_API_KEY"] = "Add your key"

# Load Embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Load Vector Store
# Make sure 'faiss_index_local' was generated by the notebook cells above
vector_store = FAISS.load_local("faiss_index_local", embeddings, allow_dangerous_deserialization=True)

# Setup LLM
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.3)

prompt_template = """You are an empathetic and professional mental health chatbot assistant.
Use the following pieces of context to answer the user's question. 
If the answer is not in the context, generally answer from your knowledge but mention that this specific information wasn't in the provided documents.
Always maintain a supportive tone.

Context: {context}

Question: {question}

Answer:"""

PROMPT = PromptTemplate(
    template=prompt_template, 
    input_variables=["context", "question"]
)

@cl.on_chat_start
async def start():
    chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
        chain_type_kwargs={"prompt": PROMPT}
    )
    cl.user_session.set("chain", chain)
    await cl.Message(content="Hello! I'm here to support you. How are you feeling today?").send()

@cl.on_message
async def main(message: cl.Message):
    chain = cl.user_session.get("chain")
    # Async callback handler for streaming if supported, though RetrievalQA is synchronous by default unless using .acall or similar with async components
    res = await chain.ainvoke(message.content)
    answer = res["result"]
    
    await cl.Message(content=answer).send()
